A lexikális szemantikai kutatások, ezen belül a disztribúciós szemantika egyre nagyobb teret nyer a számítógépes nyelvészet különböző ágaiban (pl. szinonima-detektálás, szemantikai relációk tanulása, ontológiák/lexikai adatbázisok automatikus építése, dokumentum-kategorizálás), és ennek megfelelően egyre többféle kiértékelési feladat létezik a témában (l. SemEval kampányok). Cikkünkben a lexikális behelyettesítési feladat (lexical substitution) magyarra adatptálását és két különböző megoldásának tesztelését tárgyaljuk. A lexikális behelyettesítés (McCarthy és Navigli 2007, Toral 2009, Fabre és mtsai 2014) célja olyan algoritmus megalkotása, mely képes egy lexikális egység (egyszerű szó, többszavas kifejezés) egy-egy mondatbeli előfordulását másik egységgel helyettesíteni olyan módon, hogy a mondat eredeti jelentését a lehető legjobban megőrizze. A feladat általunk kipróbált változatában az algoritmusnak kell elvégeznie a behelyettesítésre javasolt jelöltek (elsősorban, de nem kizárólag szinonimák) generálását, valamint a szövegkörnyezetbe legjobban illeszkedő lexikális egység kiválasztását. A kiértékelés során a rendszer által javasolt jelölteket annotátorok által adott válaszokkal vetjük össze. A behelyettesítési feladat magyarra alkalmazásának célja, hogy felmérjük a lexikális/disztribúciós szemantikai módszerek működésének hatékonyságát, valamint – a más nyelveken végzett kísérletekkel összevetve – képet kapjunk az esetlegesen felmerülő magyar-specifikus kihívásokról: a rendelkezésre álló erőforrásokról, illetve a nyelvi jellegzetességekből adódó problémákról.
A lexikális behelyettesítés jellemzően két részfeladtra osztható. Az első lépés a jelöltek kinyerése egy erre alkalmas jelentéstárból (általában wordnetből) vagy szinonima-adatbázisból. Bár sok kritika fogalmazódott meg a WordNet alkalmasságát illetően (elsősorban jelentésegyértelműsítési kontextusban, l. Ide és Wilks 2006, illetve a magyarra Héja és mtsai 2009), az angol nyelvű lexikális behelyettesítési verseny (SemEval 2007) során a legjobbnak bizonyult módszerek mégis mind támaszkodnak a WordNet-re. A második lépés a jelöltek rangsorolása aszerint, hogy melyik illeszkedik legjobban az adott szövegkörnyezetbe. Ez a feladat közel áll a jelentésegyértelműsítéshez, ám annotált szinonima-tár hiányában nem támaszkodhatunk felügyelt tanítási módszerekre. Lesk (1986) szótári definíciókat, Aguirre és Rigau (1996) WordNet-alapú távolsági mértékeket, Carrol és McCarthy (2000) szemantikai szelekciós információkat használ az egyértelműsítéshez. A disztribúciós szemantikában használt vektoriális szó-reprezentációk is alkalmasak rá, hogy szavak vagy nagyobb szövegegységek közötti hasonlósági mértékeket számítsunk belőlük. Egyes kutatások látens szemantikai dimenziókat alkalmaznak a szójelentések automatikus elkülönítésére és kontextusbeli egyértelműsítésére (Lin & Pantel, 2002; van de Cruys et al. 2011). A szavak elosztott reprezentációján (distributed lexical representations; word embedding) alapuló nyelvmodellek (Mikolov et al. 2013) által generált vektoriális reprezentációk is alkalmasak arra, hogy rajtuk értelmezhető közelségi metrikák alapján döntsünk a szavak szemantikai közelségéről.
Ezek a módszerek több SemEval versenyen – szóhasonlósági és szóanalógiás feladatok esetében – jól teljesítettek (Semeval 2012, 2014). A word2vec (Mikolov 2013) és a GloVe (Pennington et al. 2014) módszerek a szavakhoz – vagy tetszőleges nagyobb egységekhez – egy valós vektortérbeli vektort rendelnek, úgy, hogy az így létrejött reprezentációra két tulajdonság jellemző: egyrészt az egymáshoz közel eső szavak szemantikai illetve morfológiai értelemben is közeliek, másrészt a vektorok közötti vektoriális különbségek konzisztensek, és egyik szópárról a másikra átvihetők. Jellegzetes példa a szópárok között kinyerhető analógiás hasonlóságra : v(king) - v(queen) ~= v(man) - v(woman). Ez a két tulajdonság indokolja a módszerek közvetlen használhatóságát a szószemantikai feladatokban. A behelyettesítéses feladaton legújabban Ferret (2014) végzett kísérletet francia nyelvre a word2vec által generált reprezentáció felhasználásával. 
Kísérletünkben létrehozunk egy ilyen vektoros reprezentációt magyar szavakra, és ennek használhatóságát mindkét részfeladatra kipróbáljuk.
Másodsorban egy hibrid módszerrel próbálkozunk, mely a WordNet-beli lemmákat, illetve a köztük definiált hierarchikus kapcsolatokból származó információt kombinálja a disztribúciós szemantika és a dokumentumkategorizálás területén használt eljárásokkal. A célszó különböző jelentéseit, és az ezekhez tartozó lexikai egységeket a WordNetből nyerjük ki. A WordNet-jelentések klaszterezése után a jelentéseket körülvevő releváns csomópontok körbejárásával tematikus kategóriákat képezünk, melyekhez ezután a korpuszból gyűjtünk kategória-specifikus kontextusokat. Az így kinyert kontextusok képezik a célszó ún. egyértelműsítő vektorterét, melyen a célszót potenciálisan helyettesítő összes jelöltet elhelyezzük. Az egyértelműsítés során a jelöltek vektoros reprezentációját vetjük össze a kontextus szavaival.
Az eredmények kiértékeléséhez használt adatok elkészítésekor a McCarthy és Navigli (Semeval 2007), illetve a Fabre et. al. (SemDis 2014) által követett módszert vettük alapul. Tíz poliszém főnevet választottunk a kiértékeléshez, melyek minden jelentésükben rendelkeznek egytagú szinonimával. Feltétel volt továbbá, hogy maga a főnév, valamint szinonimái (jelentésenként legalább egy) is kellő mértékben reprezentálva legyenek a rendelkezésre álló korpuszban, hiszen elsősorban a korpusz-alapú disztribúciós komponensek teljesítményét szeretnénk kiértékelni. Korpuszként a Magyar Nemzeti Szövegtár kibővített változatát használtuk. Minden célszóhoz 10-10 példamondatot kerestünk oly módon, hogy minden szónak minden jelentése reprezentálva legyen. A célszó mondatbeli előfordulásaihoz ezután 3-3 annotátor javasolt mondatonként legfeljebb négy behelyettesíthető lexikai elemet. A rendszer által javasolt megoldásokat ezekkel a kiértékelési adatokkal vethetjük össze, figyelembe véve azt is, hogy a rendszer megoldása hány annotátor javaslatai között szerepel.

